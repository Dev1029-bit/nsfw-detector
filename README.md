# ğŸ” NSFW Image Detection Microservice (Offline + Ethical)

A clean, lightweight NSFW image classification microservice built with **FastAPI** and a **pre-trained MobileNet v2 model** (Gantman). This project is designed for **offline usage**, **ethical testing**, and **academic demonstration** only â€” no real NSFW imagery is used or supported.

---

## ğŸ“Œ Project Purpose

This microservice is built for:

* âœ… Ethical moderation testing
* âœ… Base64 image classification (safe or NSFW)
* âœ… Local-only execution (no cloud hosting)
* âœ… Academic or showcase purposes
* âŒ No real NSFW or explicit content

---

## ğŸ§  Model & Categories

The model classifies images into 5 categories:

* `drawings`
* `hentai`
* `neutral`
* `porn`
* `sexy`

Your **`nsfw_score`** is calculated as:

```python
nsfw_score = porn + sexy + hentai
```

Then classified using threshold logic from `config.json`.

---

## ğŸ—‚ï¸ Project Structure

```
nsfw_detector_core/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py           # FastAPI application entry point
â”‚   â”œâ”€â”€ model.py          # Model prediction and scoring logic
â”‚   â”œâ”€â”€ utils.py          # Base64 decoding, preprocessing, config loader
â”‚   â””â”€â”€ schemas.py        # Pydantic models for request and response
â”‚
â”œâ”€â”€ config.json           # Configuration for model thresholds and path
â”œâ”€â”€ Dockerfile            # Docker image definition
â”œâ”€â”€ requirements.txt      # Python dependencies list
â”œâ”€â”€ README.md             # Project documentation
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ saved_model.h5    # Pre-trained MobileNetV2 NSFW classifier
â”‚
â”œâ”€â”€ datasets/             # Local image datasets (used for evaluation)
â”‚   â””â”€â”€ safe/             # Manually curated safe images (e.g., from Unsplash)
â”‚       â”œâ”€â”€ safe_1.jpg
â”‚       â”œâ”€â”€ safe_2.jpg
â”‚       â””â”€â”€ ...
```

---

## âš™ï¸ Configuration: `config.json`

```json
{
  "model_path": "model/saved_model.h5",
  "nsfw_threshold": 0.4,
  "intermediate_flag_range": [0.2, 0.4]
}
```

---

## ğŸš€ Local Setup

### âœ… Install Dependencies

```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run FastAPI Server

```bash
uvicorn app.main:app --reload
```

Then visit:

```
http://localhost:8000/docs
```

---

## ğŸ³ Docker Setup (Simple Dockerfile-Only)

> Only for **local demo purposes** â€” no cloud hosting required

### 1ï¸âƒ£ Build the image

```
docker build -t nsfw-detector-app .
```

### 2ï¸âƒ£ Run the container

```
docker run -p 8000:8000 nsfw-detector-app
```

### 3ï¸âƒ£ Access the Swagger UI

```
http://localhost:8000/docs
```

---

âœ… How to Use POST /check-image

Input Format: Send a valid image (e.g., .jpg, .png) as a Base64-encoded string in a JSON payload:

{
  "user_id": "stu_3092",
  "image_base64": "<your_base64_string_here>"
}

user_id: any unique string identifying the user

image_base64: must be the raw base64 string only (exclude data:image/...;base64, prefix)

How to Convert Your Image to Base64:

Visit https://www.base64-image.de/

Upload your image file

Copy only the Base64 content (ignore the data:image/... part)

Paste the string in the image_base64 field in Swagger UI

How to Test via Swagger UI:

Go to http://localhost:8000/docs

Expand POST /check-image

Click Try it out

Fill in the JSON body with user_id and valid image_base64

Click Execute to receive classification

Expected Output:**

{
  "user_id": "stu_3092",
  "nsfw_score": 0.88,
  "label": "nsfw",
  "threshold": 0.55,
  "status": "blocked"
}

label: safe / nsfw

status: approved / flagged / blocked based on score range

threshold: fetched from config.json

---

### Example Output

```json
{
  "user_id": "stu_3092",
  "nsfw_score": 0.88,
  "label": "nsfw",
  "threshold": 0.4,
  "status": "blocked"
}
```

---

## ğŸ” Ethical Considerations

> This project does **not** involve real explicit images.

* âœ… NSFW images used are **synthetic, blurred, or anime-style**
* âŒ No real pornographic content allowed
* âœ… Safe categories include **drawings** and **neutral**
* âœ… Complies with offline moderation research policies

---

## ğŸ“ Ideal For

* Student & academic demos
* Responsible AI demonstrations
* Local moderation tools
* FastAPI + ML portfolio projects

---

## ğŸ§ª Testing Tips

* Use Swagger UI or Postman
* Supply `base64` images under 512KB
* Test `nsfw`, `flagged`, and `safe` examples


---

## ğŸ“„ License

MIT License â€“ For academic, ethical, and personal use.

---

## ğŸ™ Credits

* [Gantman NSFW Model](https://github.com/GantMan/nsfw_model)
* [FastAPI Framework](https://fastapi.tiangolo.com/)
* TensorFlow, NumPy, Pillow

---

> ğŸ”’ Built for responsible use only. No explicit content is included, supported, or allowed.
